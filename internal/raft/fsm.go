// Package raft provides the finite state machine implementations for distributed
// consensus operations in the Prism orchestration platform.
//
// This file contains the FSM implementations that maintain consistent state
// across the cluster for AI sandbox lifecycle management, placement decisions,
// and orchestration operations. The FSMs process Raft log entries to ensure
// all nodes maintain identical state for distributed coordination.
//
// FSM ARCHITECTURE:
// The system uses a hierarchical FSM structure with PrismFSM as the root that
// delegates to specialized sub-FSMs for different operational domains:
//
//   - SandboxFSM: Manages code execution sandbox environments
//   - Future: Additional FSMs for extended functionality as the platform grows
//
// COMMAND PROCESSING:
// Commands are JSON-encoded operations that specify the target FSM and action.
// Each FSM processes its own command types and maintains its own state while
// contributing to the overall cluster state for orchestration decisions.
//
// STATE PERSISTENCE:
// All FSM state is automatically persisted through Raft's log replication and
// snapshot mechanisms. State is replicated to all nodes and survives node
// restarts, providing durability for critical orchestration state.
package raft

import (
	"encoding/json"
	"fmt"
	"io"
	"sync"
	"time"
	"unicode/utf8"

	"github.com/concave-dev/prism/internal/logging"
	"github.com/hashicorp/raft"
)

const (
	// MaxCommandOutputSize defines the maximum size in bytes for command output
	// stored in ExecutionRecord. Prevents excessive memory usage from large
	// command outputs while preserving essential execution information.
	//
	// Output exceeding this limit is truncated with an indicator to signal
	// that the full output was not stored. This helps maintain cluster
	// performance and prevents memory exhaustion from verbose commands.
	//
	// NOTE: This is a temporary solution. Future versions will store only
	// output IDs in Raft and move full output to a distributed database
	// to eliminate Raft log bloat and improve cluster scalability.
	MaxCommandOutputSize = 4096 // 4KB limit for command output storage
)

// truncateOutput safely truncates command output to MaxCommandOutputSize bytes
// while preserving UTF-8 character boundaries. Appends truncation indicator
// when content exceeds the maximum size limit.
//
// Ensures stored output never exceeds memory limits while maintaining UTF-8
// validity. Critical for preventing memory exhaustion from verbose command
// outputs in distributed execution environments.
func truncateOutput(output string) string {
	if len(output) <= MaxCommandOutputSize {
		return output
	}

	// Reserve space for truncation indicator
	truncationIndicator := "...(truncated)"
	maxContentSize := MaxCommandOutputSize - len(truncationIndicator)

	// Ensure we don't split UTF-8 characters by finding valid boundary
	truncated := output[:maxContentSize]
	for len(truncated) > 0 && !utf8.ValidString(truncated) {
		truncated = truncated[:len(truncated)-1]
	}

	return truncated + truncationIndicator
}

// Scheduler defines the interface for sandbox scheduling operations in the
// distributed cluster. Provides methods for triggering placement decisions
// and managing sandbox lifecycle coordination.
//
// TODO: This interface is duplicated in internal/scheduler/scheduler.go
// Consider moving to a shared interfaces package to eliminate duplication
// and avoid circular dependencies between raft and scheduler packages.
type Scheduler interface {
	ScheduleSandbox(sandboxID string) error
	IsLeader() bool
}

// PrismFSM is the root finite state machine that coordinates all distributed
// state management operations for the Prism orchestration platform. It delegates
// to specialized sub-FSMs for different operational domains while maintaining
// overall cluster state consistency.
//
// Serves as the central coordination point for all distributed operations
// including sandbox lifecycle management, resource allocation, and future
// extensions for advanced sandbox features and cluster management capabilities.
// Processes Raft commands and ensures state consistency across nodes.
type PrismFSM struct {
	mu         sync.RWMutex // Protects all FSM state from concurrent access
	clusterID  string       // Persistent cluster identifier, generated by leader
	sandboxFSM *SandboxFSM  // Manages code execution sandbox environments
	scheduler  Scheduler    // Scheduler for triggering sandbox placement (optional)

	// Future FSM extensions for advanced sandbox capabilities:
	// networkFSM  *NetworkFSM  // Sandbox networking and isolation policies
	// storageFSM  *StorageFSM  // Persistent volumes and snapshots
	// imageFSM    *ImageFSM    // Container image management and caching
	// resourceFSM *ResourceFSM // Advanced resource allocation and limits
}

// SandboxFSM manages the complete lifecycle of code execution sandboxes in the
// distributed cluster. Tracks sandbox creation, command execution, status updates,
// and cleanup operations while coordinating with the runtime system for secure
// code execution within Firecracker VM environments.
//
// Maintains authoritative state for all sandboxes across the cluster and processes
// execution requests through secure isolation boundaries. Provides the foundation
// for safe AI-generated code execution within the distributed sandbox platform.
type SandboxFSM struct {
	sandboxes map[string]*Sandbox // sandboxID -> Sandbox state for all sandboxes in cluster

	// Execution FSM for managing command execution lifecycle
	execFSM *ExecFSM
}

// Sandbox represents the complete state of a code execution sandbox in the
// distributed cluster. Tracks lifecycle status, execution history, runtime
// configuration, and operational metadata needed for secure code execution
// and monitoring operations.
//
// Serves as the authoritative record for sandbox state that is replicated across
// all cluster nodes through Raft consensus. Contains all information needed
// for execution decisions, security enforcement, and lifecycle management.
type Sandbox struct {
	// Core identification and metadata
	ID      string    `json:"id"`      // Unique sandbox identifier
	Name    string    `json:"name"`    // Human-readable sandbox name
	Status  string    `json:"status"`  // Current status: "pending", "assigned", "ready", "stopped", "failed", "lost", "deleted"
	Created time.Time `json:"created"` // Sandbox creation timestamp
	Updated time.Time `json:"updated"` // Last status update timestamp

	// Scheduling information for placement decisions
	ScheduledNodeID string    `json:"scheduled_node_id,omitempty"` // Node selected for sandbox placement
	ScheduledAt     time.Time `json:"scheduled_at,omitempty"`      // When scheduling decision was made
	PlacementScore  float64   `json:"placement_score,omitempty"`   // Resource score of selected node at placement time

	// Execution count for summary information
	ExecCount int `json:"exec_count"` // Total number of commands executed

	// Operational metadata for monitoring and debugging
	Metadata map[string]string `json:"metadata,omitempty"` // Additional key-value metadata
}

// ExecFSM manages the complete lifecycle of command executions within sandbox
// environments. Tracks individual execution requests from creation through
// completion, providing detailed execution status and result tracking.
//
// Maintains separate execution state from sandbox lifecycle to enable multiple
// concurrent executions within a single sandbox and precise execution monitoring.
// Provides the foundation for execution history, debugging, and audit trails.
type ExecFSM struct {
	executions map[string]*Execution // execID -> Execution state for all executions
}

// Execution represents a single command execution within a sandbox environment.
// Tracks execution lifecycle from creation through completion with detailed
// status information, timing data, and output capture for monitoring and debugging.
//
// Provides granular execution tracking separate from sandbox lifecycle state,
// enabling multiple concurrent executions per sandbox and precise execution
// monitoring across the distributed cluster infrastructure.
type Execution struct {
	// Core identification and metadata
	ID        string    `json:"id"`         // Unique execution identifier
	SandboxID string    `json:"sandbox_id"` // Parent sandbox identifier
	Command   string    `json:"command"`    // Command to execute
	Status    string    `json:"status"`     // Current status: "pending", "running", "completed", "failed"
	Created   time.Time `json:"created"`    // Execution creation timestamp
	Updated   time.Time `json:"updated"`    // Last status update timestamp

	// Execution timing and results
	StartedAt   *time.Time `json:"started_at,omitempty"`   // When execution started (if running/completed/failed)
	CompletedAt *time.Time `json:"completed_at,omitempty"` // When execution finished (if completed/failed)
	Duration    int64      `json:"duration_ms"`            // Execution duration in milliseconds
	ExitCode    int        `json:"exit_code"`              // Process exit code (if completed/failed)

	// Output and error capture
	Stdout string `json:"stdout,omitempty"` // Command standard output (truncated)
	Stderr string `json:"stderr,omitempty"` // Command standard error (truncated)

	// Additional execution metadata
	Metadata map[string]string `json:"metadata,omitempty"` // Additional execution metadata
}

// Command represents a distributed operation that should be applied consistently
// across all cluster nodes through Raft consensus. Commands are JSON-encoded
// and include the target FSM and operation details.
//
// Provides the interface for all distributed state changes in the cluster.
// Commands are replicated through Raft to ensure all nodes process the same
// operations in the same order, maintaining strong consistency guarantees.
type Command struct {
	Type      string          `json:"type"`      // Command type: "sandbox", "cluster_id", "network", "storage", etc.
	Operation string          `json:"operation"` // Operation: "create", "update", "delete", "place", "set"
	Data      json.RawMessage `json:"data"`      // Operation-specific data payload
	Timestamp time.Time       `json:"timestamp"` // Command creation timestamp
	NodeID    string          `json:"node_id"`   // Originating node identifier
}

// SandboxCreateCommand represents a request to create a new sandbox in the cluster.
// Contains all information needed for sandbox creation and initial provisioning
// including execution environment configuration and operational metadata.
//
// Processed by the SandboxFSM to create new sandbox records and trigger runtime
// provisioning based on current cluster resource availability and security policies.
type SandboxCreateCommand struct {
	ID       string            `json:"id"`                 // Pre-generated sandbox ID
	Name     string            `json:"name"`               // Sandbox name
	Metadata map[string]string `json:"metadata,omitempty"` // Additional metadata
}

// SandboxExecCommand represents a request to execute a command within an existing
// sandbox environment. Contains the command to execute and execution context
// for secure command execution within isolated sandbox boundaries.
//
// Processed by the SandboxFSM to record execution intent and coordinate with
// the runtime system for actual command execution within Firecracker VM environments.
type SandboxExecCommand struct {
	SandboxID string `json:"sandbox_id"` // Target sandbox identifier
	Command   string `json:"command"`    // Command to execute
}

// SandboxScheduleCommand represents a scheduling decision for sandbox placement
// on a specific node within the cluster. Contains the selected node information
// and placement score for tracking scheduling decisions and performance analysis.
//
// Processed by the SandboxFSM to record scheduling decisions and transition
// sandboxes from "pending" to "assigned" status. Enables tracking of placement
// decisions for monitoring and debugging distributed scheduling operations.
type SandboxScheduleCommand struct {
	SandboxID      string  `json:"sandbox_id"`       // Target sandbox identifier
	SelectedNodeID string  `json:"selected_node_id"` // Node selected for placement
	PlacementScore float64 `json:"placement_score"`  // Resource score of selected node
}

// SandboxStatusUpdateCommand represents status updates from placement operations
// performed by cluster nodes. Contains new status and optional message for
// tracking sandbox lifecycle transitions and placement results.
//
// Processed by the SandboxFSM to update sandbox status based on placement
// results from nodes. Enables tracking of placement success, failure, or
// timeout scenarios for comprehensive sandbox lifecycle management.
type SandboxStatusUpdateCommand struct {
	SandboxID string `json:"sandbox_id"`        // Target sandbox identifier
	Status    string `json:"status"`            // New status: "ready", "failed", "lost"
	Message   string `json:"message,omitempty"` // Optional status details or error message
}

// SandboxStopCommand represents a request to stop a running sandbox
// and transition it to stopped state for pause/resume functionality.
// Contains stop options for graceful vs forceful shutdown behavior.
//
// Processed by the SandboxFSM to coordinate sandbox stopping with
// the runtime system. Enables pause/resume workflows and resource
// management for temporarily unused sandbox environments.
type SandboxStopCommand struct {
	SandboxID string `json:"sandbox_id"`         // Target sandbox identifier
	Graceful  bool   `json:"graceful,omitempty"` // Whether to stop gracefully (default: true)
}

// BatchSandboxCreateCommand represents a batch request to create multiple
// sandboxes in a single Raft operation. Reduces consensus overhead during
// high-load scenarios by grouping multiple create operations together.
//
// Processed by the SandboxFSM by storing all sandboxes in "pending" state
// within one distributed state change. Background scheduler handles actual
// placement and resource allocation asynchronously for better throughput.
type BatchSandboxCreateCommand struct {
	Sandboxes []SandboxCreateCommand `json:"sandboxes"` // List of sandboxes to create
}

// BatchSandboxDeleteCommand represents a batch request to delete multiple
// sandboxes in a single Raft operation. Reduces consensus overhead during
// cleanup operations by grouping multiple delete operations together.
//
// Processed by the SandboxFSM by removing all specified sandboxes from
// cluster state within one distributed operation. Maintains consistency
// while improving performance during bulk cleanup scenarios.
type BatchSandboxDeleteCommand struct {
	SandboxIDs []string `json:"sandbox_ids"` // List of sandbox IDs to delete
}

// ExecCreateCommand represents a request to create a new execution within
// a sandbox environment. Contains the command to execute and target sandbox.
//
// Processed by the ExecFSM to create new execution records in "pending" status
// and prepare for runtime execution coordination with sandbox environments.
type ExecCreateCommand struct {
	ID        string            `json:"id"`                 // Pre-generated execution ID
	SandboxID string            `json:"sandbox_id"`         // Target sandbox identifier
	Command   string            `json:"command"`            // Command to execute
	Metadata  map[string]string `json:"metadata,omitempty"` // Additional execution metadata
}

// ExecStartCommand represents a request to start a pending execution.
// Transitions execution from "pending" to "running" status and coordinates
// with runtime system for actual command execution.
type ExecStartCommand struct {
	ExecID string `json:"exec_id"` // Target execution identifier
}

// ExecCompleteCommand represents completion of a running execution.
// Transitions execution from "running" to "completed" status with results.
type ExecCompleteCommand struct {
	ExecID   string `json:"exec_id"`   // Target execution identifier
	ExitCode int    `json:"exit_code"` // Process exit code
	Stdout   string `json:"stdout"`    // Command standard output (truncated)
	Stderr   string `json:"stderr"`    // Command standard error (truncated)
	Duration int64  `json:"duration"`  // Execution duration in milliseconds
}

// ExecFailCommand represents failure of a pending or running execution.
// Transitions execution to "failed" status with error information.
type ExecFailCommand struct {
	ExecID   string `json:"exec_id"`            // Target execution identifier
	Reason   string `json:"reason"`             // Failure reason
	ExitCode int    `json:"exit_code"`          // Process exit code (if available)
	Stdout   string `json:"stdout,omitempty"`   // Command standard output (if any)
	Stderr   string `json:"stderr,omitempty"`   // Command standard error (if any)
	Duration int64  `json:"duration,omitempty"` // Execution duration in milliseconds (if available)
}

// ClusterIDCommand represents a request to set the cluster identifier.
// Contains the cluster ID that should be applied across all nodes via Raft
// consensus to ensure cluster-wide consistency and persistence.
//
// Only processed by the leader to prevent conflicts and ensure a single
// source of truth for cluster identity. The cluster ID persists through
// leadership changes, node restarts, and cluster recovery operations.
type ClusterIDCommand struct {
	ClusterID string `json:"cluster_id"` // Unique cluster identifier
}

// NewPrismFSM creates a new PrismFSM with initialized sub-FSMs for all
// operational domains. Sets up the hierarchical FSM structure needed
// for comprehensive distributed state management.
//
// Establishes the foundation for all distributed operations in the cluster
// by initializing specialized FSMs for different operational concerns.
// Each sub-FSM maintains its own state while contributing to overall coordination.
func NewPrismFSM() *PrismFSM {
	return &PrismFSM{
		sandboxFSM: NewSandboxFSM(),
		scheduler:  nil, // Will be set later via SetScheduler
	}
}

// SetScheduler sets the scheduler for automatic sandbox placement after creation.
// This allows the FSM to trigger scheduling operations when sandboxes are created
// while maintaining clean separation between FSM and scheduler concerns.
//
// Essential for automatic scheduling as it enables the FSM to trigger placement
// decisions immediately after sandbox creation without tight coupling to
// scheduler implementation details.
func (f *PrismFSM) SetScheduler(scheduler Scheduler) {
	f.mu.Lock()
	defer f.mu.Unlock()
	f.scheduler = scheduler
}

// extractScheduleID extracts sandbox ID from command result if scheduling
// should be triggered. Returns empty string if no scheduling is needed.
// Handles the complexity of result parsing and scheduling conditions.
//
// Keeps the main Apply method clean while providing comprehensive scheduling
// trigger logic with proper error handling and leader verification.
func (f *PrismFSM) extractScheduleID(cmd Command, result any) string {
	// Only trigger scheduling for sandbox creation commands
	if cmd.Operation != "create" {
		return ""
	}

	// Check if scheduler is available and we're the leader
	if f.scheduler == nil || !f.scheduler.IsLeader() {
		return ""
	}

	// Extract sandbox ID from command result
	resultMap, ok := result.(map[string]any)
	if !ok {
		return ""
	}

	sandboxID, exists := resultMap["sandbox_id"].(string)
	if !exists {
		return ""
	}

	return sandboxID
}

// triggerScheduling initiates scheduling asynchronously for the given sandbox ID.
// Spawns goroutine to avoid blocking the caller and handles scheduling errors.
//
// Called outside the FSM lock to prevent lock contention between Apply operations
// and scheduler queries that need to read FSM state.
func (f *PrismFSM) triggerScheduling(sandboxID string) {
	go func() {
		if err := f.scheduler.ScheduleSandbox(sandboxID); err != nil {
			logging.Error("FSM: Failed to trigger scheduling for sandbox %s: %v", logging.FormatSandboxID(sandboxID), err)
		}
	}()
}

// NewSandboxFSM creates a new SandboxFSM with initialized state tracking structures
// for sandbox lifecycle management and execution monitoring. Sets up the data
// structures needed for secure code execution orchestration and debugging.
//
// Initializes all tracking structures needed for sandbox lifecycle management
// including execution history for debugging and monitoring code execution
// patterns across the distributed cluster infrastructure.
func NewSandboxFSM() *SandboxFSM {
	return &SandboxFSM{
		sandboxes: make(map[string]*Sandbox),
		execFSM:   NewExecFSM(),
	}
}

// NewExecFSM creates a new ExecFSM with initialized execution tracking structures
// for command execution lifecycle management. Sets up the data structures needed
// for execution monitoring, status tracking, and result collection.
//
// Initializes execution tracking for granular command execution monitoring
// separate from sandbox lifecycle state, enabling multiple concurrent executions
// per sandbox and precise execution audit trails.
func NewExecFSM() *ExecFSM {
	return &ExecFSM{
		executions: make(map[string]*Execution),
	}
}

// processCommand routes execution commands to appropriate handler methods based
// on the operation type. Maintains execution lifecycle state consistency across
// all cluster nodes through Raft consensus.
//
// Handles all execution operations including creation, state transitions, and
// result recording for comprehensive execution tracking and monitoring.
func (e *ExecFSM) processCommand(cmd Command, sandboxes map[string]*Sandbox) any {
	switch cmd.Operation {
	case "create":
		return e.processCreateCommand(cmd, sandboxes)
	case "start":
		return e.processStartCommand(cmd)
	case "complete":
		return e.processCompleteCommand(cmd)
	case "fail":
		return e.processFailCommand(cmd)
	default:
		return fmt.Errorf("unknown execution operation: %s", cmd.Operation)
	}
}

// Apply processes committed Raft log entries and applies them to the appropriate
// sub-FSM based on the command type. Maintains consistent state across all
// cluster nodes by processing commands in the same order on every node.
//
// Critical for distributed state consistency as it ensures all nodes apply
// the same state changes in the same sequence. Routes commands to specialized
// FSMs while maintaining overall coordination and state integrity.
//
// LOCK CONTENTION FIX:
// Previous implementation held f.mu.Lock() while spawning scheduling goroutines,
// causing lock contention when the goroutine immediately called GetSandbox() which
// needs f.mu.RLock(). This created unnecessary blocking and increased Raft Apply
// tail latency on busy leaders.
//
// Current implementation:
// 1. Hold lock only during state mutations (processCommand)
// 2. Extract scheduling information while locked (extractScheduleID)
// 3. Release lock explicitly before spawning goroutines
// 4. Spawn scheduling goroutines outside lock to eliminate contention
//
// This reduces Apply latency and prevents potential deadlocks in future
// scheduler implementations that might need FSM callbacks during Apply.
func (f *PrismFSM) Apply(log *raft.Log) any {
	f.mu.Lock()

	var scheduleID string // Track sandbox ID for scheduling outside lock
	var result any

	switch log.Type {
	case raft.LogCommand:
		// Parse the command from log data
		var cmd Command
		if err := json.Unmarshal(log.Data, &cmd); err != nil {
			f.mu.Unlock()
			return fmt.Errorf("failed to unmarshal command: %w", err)
		}

		logging.Info("FSM: Processing %s %s command from node %s",
			cmd.Type, cmd.Operation, cmd.NodeID)

		// Route command to appropriate sub-FSM
		switch cmd.Type {
		case "sandbox":
			result = f.sandboxFSM.processCommand(cmd)
			scheduleID = f.extractScheduleID(cmd, result)
		case "exec":
			result = f.sandboxFSM.execFSM.processCommand(cmd, f.sandboxFSM.sandboxes)
		case "cluster_id":
			result = f.applyClusterIDCommand(cmd)
		default:
			f.mu.Unlock()
			return fmt.Errorf("unknown command type: %s", cmd.Type)
		}

	default:
		logging.Warn("FSM: Unknown log type: %v", log.Type)
		f.mu.Unlock()
		return fmt.Errorf("unknown log type: %v", log.Type)
	}

	f.mu.Unlock() // Release lock before spawning goroutine

	// Trigger scheduling outside the lock to avoid contention
	if scheduleID != "" {
		f.triggerScheduling(scheduleID)
	}

	return result
}

// Snapshot creates a point-in-time snapshot of all FSM state for log compaction
// and recovery operations. Captures the complete state of all sub-FSMs in a
// format suitable for persistence and restoration.
//
// Critical for cluster performance and storage efficiency as it allows
// Raft to compact logs while maintaining the ability to restore complete
// state for new or recovering nodes. Includes all sandbox state and metadata.
func (f *PrismFSM) Snapshot() (raft.FSMSnapshot, error) {
	f.mu.RLock()
	defer f.mu.RUnlock()

	// Create snapshot of all sub-FSM state
	snapshot := &PrismFSMSnapshot{
		ClusterID:    f.clusterID,
		SandboxState: f.sandboxFSM.getState(),
		Timestamp:    time.Now(),
	}

	logging.Info("FSM: Created snapshot with %d sandboxes",
		len(snapshot.SandboxState.Sandboxes))
	return snapshot, nil
}

// Restore rebuilds the complete FSM state from a snapshot during cluster
// recovery or new node initialization. Restores all sub-FSM state to enable
// fast state recovery without requiring full log replay.
//
// Essential for cluster scalability and recovery as it enables new nodes
// to quickly catch up to current state and allows existing nodes to recover
// efficiently after restarts or failures. Restores all sandbox and execution state.
func (f *PrismFSM) Restore(snapshot io.ReadCloser) error {
	f.mu.Lock()
	defer f.mu.Unlock()

	// Read snapshot data
	data, err := io.ReadAll(snapshot)
	if err != nil {
		return fmt.Errorf("failed to read snapshot data: %w", err)
	}
	defer snapshot.Close()

	// Parse snapshot
	var snapshotData PrismFSMSnapshot
	if err := json.Unmarshal(data, &snapshotData); err != nil {
		return fmt.Errorf("failed to unmarshal snapshot: %w", err)
	}

	// Restore cluster ID
	f.clusterID = snapshotData.ClusterID

	// Restore sandbox FSM state
	f.sandboxFSM = NewSandboxFSM()
	f.sandboxFSM.restoreState(snapshotData.SandboxState)

	logging.Info("FSM: Restored state from snapshot with cluster ID %s and %d sandboxes",
		f.clusterID, len(snapshotData.SandboxState.Sandboxes))
	return nil
}

// GetClusterID returns the current cluster identifier stored in the FSM.
// Provides thread-safe access to the cluster ID for API endpoints and
// operational queries that need cluster identification information.
//
// Returns empty string if no cluster ID has been set yet, which occurs
// until the first leader generates and applies the cluster identifier
// via Raft consensus. Essential for cluster identification operations.
func (f *PrismFSM) GetClusterID() string {
	f.mu.RLock()
	defer f.mu.RUnlock()
	return f.clusterID
}

// applyClusterIDCommand processes cluster ID commands to set the persistent
// cluster identifier via Raft consensus. Implements FSM-level protection with
// early guard to prevent overwriting existing cluster IDs and race conditions.
//
// Provides final authoritative validation as part of defense-in-depth strategy.
// Only accepts "set" operations and ensures cluster ID can only be established
// once during cluster lifecycle. Critical for maintaining cluster identity
// consistency across all nodes through leadership changes and restarts.
func (f *PrismFSM) applyClusterIDCommand(cmd Command) interface{} {
	if cmd.Operation != "set" {
		return fmt.Errorf("unsupported cluster_id operation: %s", cmd.Operation)
	}

	var clusterIDCmd ClusterIDCommand
	if err := json.Unmarshal(cmd.Data, &clusterIDCmd); err != nil {
		return fmt.Errorf("failed to unmarshal cluster ID command: %w", err)
	}

	// Reject empty input
	if clusterIDCmd.ClusterID == "" {
		return fmt.Errorf("cluster ID cannot be empty")
	}

	// Idempotency and overwrite protection
	if f.clusterID != "" {
		if f.clusterID == clusterIDCmd.ClusterID {
			logging.Info(
				"FSM: Cluster ID already set; idempotent replay accepted "+
					"(provided=%s existing=%s)",
				clusterIDCmd.ClusterID, f.clusterID,
			)
			return nil
		}
		return fmt.Errorf(
			"cluster ID already set; refusing to change (provided=%s "+
				"existing=%s)",
			clusterIDCmd.ClusterID, f.clusterID,
		)
	}

	// Establish cluster ID
	prev := f.clusterID
	f.clusterID = clusterIDCmd.ClusterID
	logging.Info(
		"FSM: Set cluster ID (provided=%s previous=%s)",
		clusterIDCmd.ClusterID, prev,
	)
	return nil
}

// getState returns the complete state of the SandboxFSM for snapshot operations.
// Provides a serializable representation of all sandbox state and execution
// history for persistence and recovery operations.
//
// Essential for maintaining state consistency across cluster restarts and
// enabling fast recovery without full log replay. Captures all sandbox
// lifecycle and execution tracking information.
func (s *SandboxFSM) getState() *SandboxFSMState {
	return &SandboxFSMState{
		Sandboxes:  copySandboxMap(s.sandboxes),
		Executions: copyExecutionMap(s.execFSM.executions),
	}
}

// restoreState rebuilds the SandboxFSM from snapshot data during recovery
// operations. Restores all sandbox state and execution history from
// persistent storage for complete state recovery.
//
// Critical for cluster recovery operations as it enables fast state
// restoration without requiring full log replay. Maintains all sandbox
// lifecycle and execution tracking information across restarts.
func (s *SandboxFSM) restoreState(state *SandboxFSMState) {
	s.sandboxes = copySandboxMap(state.Sandboxes)
	s.execFSM.executions = copyExecutionMap(state.Executions)
}

// ============================================================================
// SANDBOX FSM COMMAND PROCESSING - Handle sandbox lifecycle operations
// ============================================================================

// processCommand handles sandbox-specific commands including creation, execution,
// and deletion operations. Maintains sandbox state consistency and coordinates
// with the runtime system for secure code execution within isolated environments.
//
// Core of the sandbox lifecycle management system that processes all distributed
// sandbox operations. Integrates with execution runtime for secure code execution
// and maintains comprehensive state tracking for monitoring and debugging.
func (s *SandboxFSM) processCommand(cmd Command) any {
	switch cmd.Operation {
	case "create":
		return s.processCreateCommand(cmd)
	case "batch_create":
		return s.processBatchCreateCommand(cmd)
	case "schedule":
		return s.processScheduleCommand(cmd)
	case "status_update":
		return s.processStatusUpdateCommand(cmd)
	case "stop":
		return s.processStopCommand(cmd)
	case "delete":
		return s.processDeleteCommand(cmd)
	case "batch_delete":
		return s.processBatchDeleteCommand(cmd)
	default:
		return fmt.Errorf("unknown sandbox operation: %s", cmd.Operation)
	}
}

// processCreateCommand handles sandbox creation requests by creating new sandbox
// records with initial status. The sandbox will be provisioned by the runtime
// system based on current cluster resource availability and security policies.
//
// Establishes the initial sandbox record in the distributed state and prepares
// for runtime provisioning. Creates sandboxes in "pending" status to allow for
// asynchronous provisioning and initialization of secure execution environments.
func (s *SandboxFSM) processCreateCommand(cmd Command) any {
	var createCmd SandboxCreateCommand
	if err := json.Unmarshal(cmd.Data, &createCmd); err != nil {
		return fmt.Errorf("failed to unmarshal create command: %w", err)
	}

	// Use pre-generated sandbox ID from the command
	sandboxID := createCmd.ID
	if sandboxID == "" {
		return fmt.Errorf("sandbox ID is required in create command")
	}

	// Check for duplicate sandbox names - names must be unique
	for _, existingSandbox := range s.sandboxes {
		if existingSandbox.Name == createCmd.Name {
			return fmt.Errorf("sandbox name '%s' already exists", createCmd.Name)
		}
	}

	// Create new sandbox record
	sandbox := &Sandbox{
		ID:       sandboxID,
		Name:     createCmd.Name,
		Status:   "pending",
		Created:  time.Now(),
		Updated:  time.Now(),
		Metadata: createCmd.Metadata,
	}

	// Store sandbox in FSM state
	s.sandboxes[sandboxID] = sandbox

	logging.Info("SandboxFSM: Created sandbox %s (%s)",
		sandboxID, createCmd.Name)

	return map[string]any{
		"sandbox_id": sandboxID,
		"status":     "pending",
	}
}

// processDeleteCommand handles sandbox deletion requests by removing sandbox
// records from the distributed state and cleaning up associated execution
// history and runtime resources.
//
// Provides clean sandbox removal with proper state cleanup across the cluster.
// Removes execution history and coordinates with runtime system for VM cleanup
// to maintain accurate cluster resource accounting and operational state.
func (s *SandboxFSM) processDeleteCommand(cmd Command) any {
	var deleteCmd struct {
		SandboxID string `json:"sandbox_id"`
	}
	if err := json.Unmarshal(cmd.Data, &deleteCmd); err != nil {
		return fmt.Errorf("failed to unmarshal delete command: %w", err)
	}

	// Find target sandbox
	sandbox, exists := s.sandboxes[deleteCmd.SandboxID]
	if !exists {
		return fmt.Errorf("sandbox not found: %s", deleteCmd.SandboxID)
	}

	// Validate sandbox can be deleted - only allow deletion of stopped sandboxes
	// This prevents state corruption even if HTTP handler validation is bypassed
	validDeleteStates := map[string]bool{
		"stopped": true,
		"failed":  true,
		"lost":    true,
	}
	if !validDeleteStates[sandbox.Status] {
		return fmt.Errorf("cannot delete sandbox %s in status %s - must be stopped first", deleteCmd.SandboxID, sandbox.Status)
	}

	// Remove sandbox from state
	delete(s.sandboxes, deleteCmd.SandboxID)

	logging.Info("SandboxFSM: Deleted sandbox %s", logging.FormatSandboxID(deleteCmd.SandboxID))

	// TODO: Coordinate with runtime system for VM cleanup and resource deallocation

	return map[string]any{
		"sandbox_id": deleteCmd.SandboxID,
		"status":     "deleted",
	}
}

// processBatchCreateCommand handles batch sandbox creation by storing multiple
// sandboxes in "pending" state within a single Raft operation. Provides
// significant performance improvements during high-load scenarios by reducing
// consensus overhead while maintaining consistency guarantees.
//
// Essential for high-throughput ingestion where multiple sandboxes need to be
// accepted quickly. Background scheduler handles actual placement and resource
// allocation asynchronously for better system responsiveness under load.
func (s *SandboxFSM) processBatchCreateCommand(cmd Command) any {
	var batchCmd BatchSandboxCreateCommand
	if err := json.Unmarshal(cmd.Data, &batchCmd); err != nil {
		return fmt.Errorf("failed to unmarshal batch create command: %w", err)
	}

	results := make([]map[string]any, 0, len(batchCmd.Sandboxes))
	successCount := 0
	failureCount := 0

	// Process each sandbox in the batch directly (single Raft operation)
	for _, sandboxCmd := range batchCmd.Sandboxes {
		// Validate sandbox doesn't already exist
		if _, exists := s.sandboxes[sandboxCmd.ID]; exists {
			logging.Warn("SandboxFSM: Sandbox %s already exists in batch create",
				logging.FormatSandboxID(sandboxCmd.ID))
			results = append(results, map[string]any{
				"sandbox_id": sandboxCmd.ID,
				"status":     "failed",
				"error":      "sandbox already exists",
			})
			failureCount++
			continue
		}

		// Create new sandbox directly in "pending" state
		sandbox := &Sandbox{
			ID:       sandboxCmd.ID,
			Name:     sandboxCmd.Name,
			Status:   "pending", // Background scheduler will handle placement
			Metadata: sandboxCmd.Metadata,
			Created:  cmd.Timestamp,
			Updated:  cmd.Timestamp,
		}

		// Add to FSM state
		s.sandboxes[sandboxCmd.ID] = sandbox

		logging.Info("SandboxFSM: Created sandbox %s (name: %s) in batch",
			logging.FormatSandboxID(sandboxCmd.ID), sandboxCmd.Name)

		results = append(results, map[string]any{
			"sandbox_id":   sandboxCmd.ID,
			"sandbox_name": sandboxCmd.Name,
			"status":       "pending",
		})
		successCount++
	}

	logging.Info("SandboxFSM: Processed batch create - %d succeeded, %d failed",
		successCount, failureCount)

	return map[string]any{
		"operation":     "batch_create",
		"total":         len(batchCmd.Sandboxes),
		"success_count": successCount,
		"failure_count": failureCount,
		"results":       results,
	}
}

// processBatchDeleteCommand handles batch sandbox deletion by removing multiple
// sandboxes from cluster state within a single Raft operation. Maintains
// consistency while improving performance during bulk cleanup scenarios.
//
// Essential for efficient cleanup operations where multiple sandboxes need to
// be removed simultaneously. Validates each deletion and maintains the same
// semantic guarantees as individual deletes while reducing consensus overhead.
func (s *SandboxFSM) processBatchDeleteCommand(cmd Command) any {
	var batchCmd BatchSandboxDeleteCommand
	if err := json.Unmarshal(cmd.Data, &batchCmd); err != nil {
		return fmt.Errorf("failed to unmarshal batch delete command: %w", err)
	}

	results := make([]map[string]any, 0, len(batchCmd.SandboxIDs))
	successCount := 0
	failureCount := 0

	// Validate delete states (same as individual delete logic)
	validDeleteStates := map[string]bool{
		"stopped": true,
		"failed":  true,
		"lost":    true,
	}

	// Process each sandbox deletion in the batch directly
	for _, sandboxID := range batchCmd.SandboxIDs {
		// Find target sandbox
		sandbox, exists := s.sandboxes[sandboxID]
		if !exists {
			logging.Warn("SandboxFSM: Sandbox %s not found in batch delete",
				logging.FormatSandboxID(sandboxID))
			results = append(results, map[string]any{
				"sandbox_id": sandboxID,
				"status":     "failed",
				"error":      "sandbox not found",
			})
			failureCount++
			continue
		}

		// Validate sandbox can be deleted
		if !validDeleteStates[sandbox.Status] {
			logging.Warn("SandboxFSM: Cannot delete sandbox %s in status %s",
				logging.FormatSandboxID(sandboxID), sandbox.Status)
			results = append(results, map[string]any{
				"sandbox_id": sandboxID,
				"status":     "failed",
				"error":      fmt.Sprintf("cannot delete sandbox in status %s", sandbox.Status),
			})
			failureCount++
			continue
		}

		// Remove sandbox from state
		delete(s.sandboxes, sandboxID)

		logging.Info("SandboxFSM: Deleted sandbox %s in batch",
			logging.FormatSandboxID(sandboxID))

		results = append(results, map[string]any{
			"sandbox_id": sandboxID,
			"status":     "deleted",
		})
		successCount++
	}

	logging.Info("SandboxFSM: Processed batch delete - %d succeeded, %d failed",
		successCount, failureCount)

	return map[string]any{
		"operation":     "batch_delete",
		"total":         len(batchCmd.SandboxIDs),
		"success_count": successCount,
		"failure_count": failureCount,
		"results":       results,
	}
}

// processScheduleCommand handles sandbox scheduling decisions by updating
// sandbox state with selected node and placement information. Transitions
// sandboxes from "pending" to "assigned" status for tracking placement decisions.
//
// Critical for distributed scheduling as it records placement decisions in
// the cluster state for monitoring and debugging. Updates sandbox with
// scheduling metadata for operational visibility and placement tracking.
func (s *SandboxFSM) processScheduleCommand(cmd Command) any {
	var schedCmd SandboxScheduleCommand
	if err := json.Unmarshal(cmd.Data, &schedCmd); err != nil {
		return fmt.Errorf("failed to unmarshal schedule command: %w", err)
	}

	// Find target sandbox
	sandbox, exists := s.sandboxes[schedCmd.SandboxID]
	if !exists {
		return fmt.Errorf("sandbox not found: %s", schedCmd.SandboxID)
	}

	// Validate current state - only schedule sandboxes in "pending" status
	if sandbox.Status != "pending" {
		return fmt.Errorf("sandbox %s cannot be scheduled from status %s",
			schedCmd.SandboxID, sandbox.Status)
	}

	// Update sandbox with scheduling information
	sandbox.Status = "assigned"
	sandbox.ScheduledNodeID = schedCmd.SelectedNodeID
	sandbox.ScheduledAt = time.Now()
	sandbox.PlacementScore = schedCmd.PlacementScore
	sandbox.Updated = time.Now()

	logging.Info("SandboxFSM: Scheduled sandbox %s on node %s (score: %.1f)",
		schedCmd.SandboxID, schedCmd.SelectedNodeID, schedCmd.PlacementScore)

	return map[string]any{
		"sandbox_id":      schedCmd.SandboxID,
		"status":          "assigned",
		"scheduled_node":  schedCmd.SelectedNodeID,
		"placement_score": schedCmd.PlacementScore,
	}
}

// processStatusUpdateCommand handles placement result updates from nodes
// after placement operations complete. Updates sandbox status based on
// placement success, failure, or timeout scenarios.
//
// Essential for sandbox lifecycle tracking as it processes placement results
// and transitions sandboxes to final states. Enables monitoring of placement
// success rates and debugging of failed placement operations.
func (s *SandboxFSM) processStatusUpdateCommand(cmd Command) any {
	var statusCmd SandboxStatusUpdateCommand
	if err := json.Unmarshal(cmd.Data, &statusCmd); err != nil {
		return fmt.Errorf("failed to unmarshal status update command: %w", err)
	}

	// Find target sandbox
	sandbox, exists := s.sandboxes[statusCmd.SandboxID]
	if !exists {
		return fmt.Errorf("sandbox not found: %s", statusCmd.SandboxID)
	}

	// Validate status transition - only update from "assigned" status
	if sandbox.Status != "assigned" {
		return fmt.Errorf("sandbox %s cannot update status from %s to %s",
			statusCmd.SandboxID, sandbox.Status, statusCmd.Status)
	}

	// Validate target status
	validStatuses := map[string]bool{
		"ready":  true,
		"failed": true,
		"lost":   true,
	}
	if !validStatuses[statusCmd.Status] {
		return fmt.Errorf("invalid status update: %s", statusCmd.Status)
	}

	// Update sandbox status
	previousStatus := sandbox.Status
	sandbox.Status = statusCmd.Status
	sandbox.Updated = time.Now()

	logging.Info("SandboxFSM: Updated sandbox %s status from %s to %s on node %s: %s",
		statusCmd.SandboxID, previousStatus, statusCmd.Status,
		sandbox.ScheduledNodeID, statusCmd.Message)

	return map[string]any{
		"sandbox_id":      statusCmd.SandboxID,
		"previous_status": previousStatus,
		"new_status":      statusCmd.Status,
		"message":         statusCmd.Message,
	}
}

// processStopCommand handles sandbox stop requests by transitioning running
// sandboxes to stopped state for pause/resume functionality. Coordinates
// with runtime system to gracefully halt VM execution while preserving state.
//
// IMPORTANT: This method is called ONLY AFTER the scheduled node has confirmed
// successful stop via gRPC. The API handler orchestrates the stop sequence:
// 1. gRPC StopSandbox call to the scheduled node
// 2. Node confirmation of successful stop
// 3. Only then is this Raft command submitted for state update
//
// This ensures distributed consistency where Raft state changes occur only
// after actual node-level stop operations complete, mirroring the creation
// semantics where placement acknowledgment precedes state transitions.
//
// Essential for resource management as it enables pausing unused sandboxes
// to free resources while maintaining the ability to resume execution later.
// Validates that only ready sandboxes can be stopped.
func (s *SandboxFSM) processStopCommand(cmd Command) any {
	var stopCmd SandboxStopCommand
	if err := json.Unmarshal(cmd.Data, &stopCmd); err != nil {
		return fmt.Errorf("failed to unmarshal stop command: %w", err)
	}

	// Find target sandbox
	sandbox, exists := s.sandboxes[stopCmd.SandboxID]
	if !exists {
		return fmt.Errorf("sandbox not found: %s", stopCmd.SandboxID)
	}

	// Validate current state - only stop ready sandboxes
	validStopStates := map[string]bool{
		"ready": true, // Ready sandboxes can be stopped
	}
	if !validStopStates[sandbox.Status] {
		return fmt.Errorf("sandbox %s cannot be stopped from status %s",
			stopCmd.SandboxID, sandbox.Status)
	}

	// Update sandbox status to stopped
	previousStatus := sandbox.Status
	sandbox.Status = "stopped"
	sandbox.Updated = time.Now()

	// TODO: Coordinate with runtime system for actual VM pause/stop operation
	// This will integrate with Firecracker runtime to pause VM execution
	// while preserving memory state for fast resume operations

	logging.Info("SandboxFSM: Stopped sandbox %s from %s (graceful: %v)",
		stopCmd.SandboxID, previousStatus, stopCmd.Graceful)

	return map[string]any{
		"sandbox_id":      stopCmd.SandboxID,
		"previous_status": previousStatus,
		"new_status":      "stopped",
		"graceful":        stopCmd.Graceful,
	}
}

// GetSandboxes returns a copy of all sandboxes in the cluster for monitoring
// and query operations. Provides read-only access to sandbox state without
// affecting FSM consistency or requiring distributed operations.
//
// Enables monitoring systems and administrative tools to query current
// sandbox state across the cluster. Returns consistent point-in-time
// snapshots of sandbox information for operational visibility.
func (f *PrismFSM) GetSandboxes() map[string]*Sandbox {
	f.mu.RLock()
	defer f.mu.RUnlock()

	return copySandboxMap(f.sandboxFSM.sandboxes)
}

// GetSandbox returns information for a specific sandbox by ID. Provides
// read-only access to individual sandbox state for monitoring and
// administrative operations.
//
// Enables targeted sandbox queries for detailed status information
// and operational monitoring. Returns nil if sandbox is not found.
func (f *PrismFSM) GetSandbox(sandboxID string) *Sandbox {
	f.mu.RLock()
	defer f.mu.RUnlock()

	if sandbox, exists := f.sandboxFSM.sandboxes[sandboxID]; exists {
		return copySandbox(sandbox)
	}
	return nil
}

// GetExecutions returns a copy of all executions in the cluster for monitoring
// and query operations. Provides read-only access to execution state without
// affecting FSM consistency or requiring distributed operations.
//
// Enables monitoring systems and administrative tools to query current
// execution state across the cluster. Returns consistent point-in-time
// snapshots of execution information for operational visibility.
func (f *PrismFSM) GetExecutions() map[string]*Execution {
	f.mu.RLock()
	defer f.mu.RUnlock()

	return copyExecutionMap(f.sandboxFSM.execFSM.executions)
}

// GetExecution returns information for a specific execution by ID. Provides
// read-only access to individual execution state for monitoring and
// administrative operations.
//
// Enables targeted execution queries for detailed status information
// and operational monitoring. Returns nil if execution is not found.
func (f *PrismFSM) GetExecution(execID string) *Execution {
	f.mu.RLock()
	defer f.mu.RUnlock()

	if execution, exists := f.sandboxFSM.execFSM.executions[execID]; exists {
		return copyExecution(execution)
	}
	return nil
}

// GetExecutionsBySandbox returns all executions for a specific sandbox.
// Provides read-only access to execution state filtered by sandbox ID
// for monitoring and administrative operations.
//
// Enables targeted queries for all executions within a specific sandbox
// environment for debugging and execution history analysis.
func (f *PrismFSM) GetExecutionsBySandbox(sandboxID string) []*Execution {
	f.mu.RLock()
	defer f.mu.RUnlock()

	var executions []*Execution
	for _, execution := range f.sandboxFSM.execFSM.executions {
		if execution.SandboxID == sandboxID {
			executions = append(executions, copyExecution(execution))
		}
	}
	return executions
}

// LogCurrentState logs FSM state information for debugging and development
// monitoring. Provides visibility into sandbox state across cluster nodes
// for troubleshooting and development purposes.
//
// Called periodically by the raft manager when DEBUG mode is enabled.
// Focuses on sandbox state for distributed orchestration monitoring.
func (f *PrismFSM) LogCurrentState(nodeID string) {
	f.mu.RLock()
	defer f.mu.RUnlock()

	// Focus on sandbox state for orchestration monitoring
	// TODO: Add other FSM states (networking, storage, images) as they're implemented
	sandboxCount := len(f.sandboxFSM.sandboxes)
	sandboxStatusCounts := make(map[string]int)
	sandboxExecCounts := make(map[string]int)

	for _, sandbox := range f.sandboxFSM.sandboxes {
		sandboxStatusCounts[sandbox.Status]++
		if sandbox.ExecCount > 0 {
			sandboxExecCounts["with_executions"]++
		} else {
			sandboxExecCounts["no_executions"]++
		}
	}

	logging.Debug("FSM State on %s: %d sandboxes, status: %v, executions: %v",
		nodeID, sandboxCount, sandboxStatusCounts, sandboxExecCounts)
}

// ============================================================================
// EXECUTION FSM COMMAND HANDLERS - Command execution lifecycle management
// ============================================================================

// processCreateCommand handles execution creation requests by creating new
// execution records in "pending" status. Validates target sandbox exists
// and is in "ready" state before creating execution tracking.
//
// Establishes execution tracking separate from sandbox lifecycle to enable
// multiple concurrent executions per sandbox and precise execution monitoring.
func (e *ExecFSM) processCreateCommand(cmd Command, sandboxes map[string]*Sandbox) any {
	var createCmd ExecCreateCommand
	if err := json.Unmarshal(cmd.Data, &createCmd); err != nil {
		return fmt.Errorf("failed to unmarshal exec create command: %w", err)
	}

	// Validate target sandbox exists
	sandbox, exists := sandboxes[createCmd.SandboxID]
	if !exists {
		return fmt.Errorf("sandbox not found: %s", createCmd.SandboxID)
	}

	// Validate sandbox state - only execute in ready sandboxes
	if sandbox.Status != "ready" {
		return fmt.Errorf("sandbox %s cannot execute commands from status %s (must be ready)",
			createCmd.SandboxID, sandbox.Status)
	}

	// Create new execution record
	execution := &Execution{
		ID:        createCmd.ID,
		SandboxID: createCmd.SandboxID,
		Command:   createCmd.Command,
		Status:    "pending",
		Created:   time.Now(),
		Updated:   time.Now(),
		Metadata:  createCmd.Metadata,
	}

	// Store execution record
	e.executions[createCmd.ID] = execution

	// Increment sandbox execution count for any execution request
	if sandbox, exists := sandboxes[createCmd.SandboxID]; exists {
		sandbox.ExecCount++
		sandbox.Updated = time.Now()
		logging.Debug("ExecFSM: Incremented ExecCount for sandbox %s to %d",
			createCmd.SandboxID, sandbox.ExecCount)
	}

	logging.Info("ExecFSM: Created execution %s for sandbox %s: %s",
		createCmd.ID, createCmd.SandboxID, createCmd.Command)

	return map[string]any{
		"exec_id":    createCmd.ID,
		"sandbox_id": createCmd.SandboxID,
		"status":     "pending",
	}
}

// processStartCommand handles execution start requests by transitioning
// executions from "pending" to "running" status. Records start timestamp
// and coordinates with runtime system for actual command execution.
func (e *ExecFSM) processStartCommand(cmd Command) any {
	var startCmd ExecStartCommand
	if err := json.Unmarshal(cmd.Data, &startCmd); err != nil {
		return fmt.Errorf("failed to unmarshal exec start command: %w", err)
	}

	// Find target execution
	execution, exists := e.executions[startCmd.ExecID]
	if !exists {
		return fmt.Errorf("execution not found: %s", startCmd.ExecID)
	}

	// Validate current state - only start pending executions
	if execution.Status != "pending" {
		return fmt.Errorf("execution %s cannot be started from status %s",
			startCmd.ExecID, execution.Status)
	}

	// Update execution status to running
	now := time.Now()
	execution.Status = "running"
	execution.StartedAt = &now
	execution.Updated = now

	logging.Info("ExecFSM: Started execution %s", logging.FormatExecID(startCmd.ExecID))

	return map[string]any{
		"exec_id": startCmd.ExecID,
		"status":  "running",
	}
}

// processCompleteCommand handles execution completion by transitioning
// executions from "running" to "completed" status with results.
// Records completion timestamp, exit code, and output data.
func (e *ExecFSM) processCompleteCommand(cmd Command) any {
	var completeCmd ExecCompleteCommand
	if err := json.Unmarshal(cmd.Data, &completeCmd); err != nil {
		return fmt.Errorf("failed to unmarshal exec complete command: %w", err)
	}

	// Find target execution
	execution, exists := e.executions[completeCmd.ExecID]
	if !exists {
		return fmt.Errorf("execution not found: %s", completeCmd.ExecID)
	}

	// Validate current state - only complete running executions
	if execution.Status != "running" {
		return fmt.Errorf("execution %s cannot be completed from status %s",
			completeCmd.ExecID, execution.Status)
	}

	// Update execution with completion data
	now := time.Now()
	execution.Status = "completed"
	execution.CompletedAt = &now
	execution.Updated = now
	execution.ExitCode = completeCmd.ExitCode
	execution.Duration = completeCmd.Duration

	// TODO: Apply truncation to prevent large outputs from bloating memory and Raft logs
	// Should call truncateOutput() with MaxCommandOutputSize before assignment
	// TODO: Later this will be stored to external db instead of Raft state
	execution.Stdout = completeCmd.Stdout
	execution.Stderr = completeCmd.Stderr

	logging.Info("ExecFSM: Completed execution %s with exit code %d",
		completeCmd.ExecID, completeCmd.ExitCode)

	return map[string]any{
		"exec_id":   completeCmd.ExecID,
		"status":    "completed",
		"exit_code": completeCmd.ExitCode,
		"duration":  completeCmd.Duration,
	}
}

// processFailCommand handles execution failure by transitioning executions
// from "pending" or "running" to "failed" status with error information.
// Records failure reason and any available output data.
func (e *ExecFSM) processFailCommand(cmd Command) any {
	var failCmd ExecFailCommand
	if err := json.Unmarshal(cmd.Data, &failCmd); err != nil {
		return fmt.Errorf("failed to unmarshal exec fail command: %w", err)
	}

	// Find target execution
	execution, exists := e.executions[failCmd.ExecID]
	if !exists {
		return fmt.Errorf("execution not found: %s", failCmd.ExecID)
	}

	// Validate current state - only fail pending or running executions
	validFailStates := map[string]bool{
		"pending": true,
		"running": true,
	}
	if !validFailStates[execution.Status] {
		return fmt.Errorf("execution %s cannot be failed from status %s",
			failCmd.ExecID, execution.Status)
	}

	// Update execution with failure data
	now := time.Now()
	execution.Status = "failed"
	execution.CompletedAt = &now
	execution.Updated = now
	execution.ExitCode = failCmd.ExitCode
	if failCmd.Duration > 0 {
		execution.Duration = failCmd.Duration
	}
	if failCmd.Stdout != "" {
		// TODO: Apply truncation to prevent large outputs from bloating memory and Raft logs
		// Should call truncateOutput() with MaxCommandOutputSize before assignment
		// TODO: Later this will be stored to external db instead of Raft state
		execution.Stdout = failCmd.Stdout
	}
	if failCmd.Stderr != "" {
		// TODO: Apply truncation to prevent large outputs from bloating memory and Raft logs
		// Should call truncateOutput() with MaxCommandOutputSize before assignment
		// TODO: Later this will be stored to external db instead of Raft state
		execution.Stderr = failCmd.Stderr
	}

	logging.Info("ExecFSM: Failed execution %s: %s", logging.FormatExecID(failCmd.ExecID), failCmd.Reason)

	return map[string]any{
		"exec_id":   failCmd.ExecID,
		"status":    "failed",
		"reason":    failCmd.Reason,
		"exit_code": failCmd.ExitCode,
	}
}

// ============================================================================
// SNAPSHOT STRUCTURES - State persistence and recovery support
// ============================================================================

// PrismFSMSnapshot represents a complete snapshot of all FSM state for
// persistence and recovery operations. Contains serializable state from
// all sub-FSMs in a format suitable for storage and restoration.
type PrismFSMSnapshot struct {
	ClusterID    string           `json:"cluster_id"`    // Persistent cluster identifier
	SandboxState *SandboxFSMState `json:"sandbox_state"` // Complete sandbox FSM state
	Timestamp    time.Time        `json:"timestamp"`     // Snapshot creation time
}

// SandboxFSMState represents the complete state of the SandboxFSM for snapshot
// operations. Contains all sandbox records and execution history in a
// serializable format for persistence and recovery operations.
type SandboxFSMState struct {
	Sandboxes  map[string]*Sandbox   `json:"sandboxes"`  // All sandbox records
	Executions map[string]*Execution `json:"executions"` // All execution records
}

// Persist saves the snapshot data to the provided sink for durable storage.
// Serializes all FSM state to JSON format for efficient storage and
// recovery operations across cluster restarts and scaling events.
//
// Critical for cluster durability as it ensures complete state is properly
// stored to disk for future recovery operations and log compaction.
// Handles serialization errors gracefully to prevent snapshot corruption.
func (s *PrismFSMSnapshot) Persist(sink raft.SnapshotSink) error {
	defer sink.Close()

	// TODO: Call sink.Cancel() on errors to ensure Raft discards corrupted snapshots
	// TODO: Snapshot errors don't bubble up to application - users won't see failures

	// Serialize snapshot to JSON
	data, err := json.Marshal(s)
	if err != nil {
		return fmt.Errorf("failed to marshal snapshot: %w", err)
	}

	// Write to sink
	if _, err := sink.Write(data); err != nil {
		return fmt.Errorf("failed to write snapshot: %w", err)
	}

	logging.Info("FSM: Persisted snapshot with %d sandboxes",
		len(s.SandboxState.Sandboxes))
	return nil
}

// Release cleans up resources associated with the snapshot when it's no longer
// needed. Called by Raft when the snapshot has been successfully persisted
// or when the snapshot operation is cancelled.
//
// Currently no cleanup is needed but provides the hook for future resource
// management as the FSM state becomes more complex with additional sub-FSMs.
func (s *PrismFSMSnapshot) Release() {
	// No resources to clean up currently
}

// ============================================================================
// UTILITY FUNCTIONS - State copying and management helpers
// ============================================================================

// copySandbox creates a deep copy of a sandbox for safe read access without
// affecting FSM state consistency. Prevents external modifications from
// corrupting the authoritative FSM state.
func copySandbox(sandbox *Sandbox) *Sandbox {
	if sandbox == nil {
		return nil
	}

	copy := *sandbox

	// Deep copy metadata map
	if sandbox.Metadata != nil {
		copy.Metadata = make(map[string]string)
		for k, v := range sandbox.Metadata {
			copy.Metadata[k] = v
		}
	}

	return &copy
}

// copySandboxMap creates a deep copy of the sandboxes map for safe read access
// without affecting FSM state consistency. Essential for snapshot operations
// and external queries that should not modify authoritative state.
func copySandboxMap(sandboxes map[string]*Sandbox) map[string]*Sandbox {
	copy := make(map[string]*Sandbox)
	for k, v := range sandboxes {
		copy[k] = copySandbox(v)
	}
	return copy
}

// copyExecution creates a deep copy of an execution record for read operations.
// Ensures execution state data is safely accessible without affecting FSM
// consistency or concurrent access to execution tracking structures.
func copyExecution(execution *Execution) *Execution {
	if execution == nil {
		return nil
	}

	copy := *execution

	// Deep copy metadata map
	if execution.Metadata != nil {
		copy.Metadata = make(map[string]string)
		for k, v := range execution.Metadata {
			copy.Metadata[k] = v
		}
	}

	// Copy time pointers
	if execution.StartedAt != nil {
		startedAt := *execution.StartedAt
		copy.StartedAt = &startedAt
	}
	if execution.CompletedAt != nil {
		completedAt := *execution.CompletedAt
		copy.CompletedAt = &completedAt
	}

	return &copy
}

// copyExecutionMap creates a deep copy of execution map for read operations.
// Ensures execution state data is safely accessible without affecting FSM
// consistency or concurrent access to execution tracking structures.
func copyExecutionMap(executions map[string]*Execution) map[string]*Execution {
	copy := make(map[string]*Execution)
	for k, v := range executions {
		copy[k] = copyExecution(v)
	}
	return copy
}
